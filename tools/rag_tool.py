import os
import time
from typing import Dict, Any, List, Optional
from core import Tool, ToolParameter, tool_action, OpenAICompatibleLLM
from memory import create_rag_pipeline

class RAGTool(Tool):
    def __init__(
        self,
        knowledge_base_path: str = "./knowledge_base",
        collection_name: str = "rag_knowledge_base",
        rag_namespace: str = "default",
        expandable: bool = False
    ):
        super().__init__(
            name="rag",
            description="RAGå·¥å…·ï¼Œæ”¯æŒå¤šæ ¼å¼æ–‡æ¡£æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæä¾›æ™ºèƒ½é—®ç­”èƒ½åŠ›",
            expandable=expandable
        )
        self.knowledge_base_path = os.path.abspath(knowledge_base_path)
        self.collection_name = collection_name
        self.rag_namespace = rag_namespace
        self._pipelines: Dict[str, Dict[str, Any]] = {}
        os.makedirs(knowledge_base_path, exist_ok=True)
        self._init_components()
    
    def _init_components(self):
        try:
            default_pipeline = create_rag_pipeline(
                collection_name=self.collection_name,
                rag_namespace=self.rag_namespace
            )
            self._pipelines[self.rag_namespace] = default_pipeline
            self.llm = OpenAICompatibleLLM()
            self.initialized = True
            print(f"[RAGTool] å·²æˆåŠŸåˆå§‹åŒ–ï¼šnamespace={self.rag_namespace}, collection={self.collection_name}")      
        except Exception as e:
            self.initialized = False
            print(f"[RAGTool] â›”\x20åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")

    def _get_pipeline(self, namespace: Optional[str] = None) -> Dict[str, Any]:
        target_ns = namespace or self.rag_namespace
        if target_ns in self._pipelines:
            return self._pipelines[target_ns]
        pipeline = create_rag_pipeline(
            collection_name=self.collection_name,
            rag_namespace=target_ns
        )
        self._pipelines[target_ns] = pipeline
        return pipeline

    def run(self, parameters: Dict[str, Any]) -> str:
        if not self.validate_parameters(parameters):
            return "å‚æ•°éªŒè¯å¤±è´¥ï¼šç¼ºå°‘å¿…éœ€çš„å‚æ•°"
        if not self.initialized:
            return f"RAGå·¥å…·æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®"
        action = parameters.get("action")
        if action == "add_document":
            return self._add_document(
                file_path=parameters.get("file_path"),
                document_id=parameters.get("document_id"),
                namespace=parameters.get("namespace", "default"),
                chunk_size=parameters.get("chunk_size", 800),
                chunk_overlap=parameters.get("chunk_overlap", 100)
            )
        elif action == "add_text":
            return self._add_text(
                text=parameters.get("text"),
                document_id=parameters.get("document_id"),
                namespace=parameters.get("namespace", "default"),
                chunk_size=parameters.get("chunk_size", 800),
                chunk_overlap=parameters.get("chunk_overlap", 100)
            )
        elif action == "ask":
            question = parameters.get("question") or parameters.get("query")
            return self._ask(
                question=question,
                limit=parameters.get("limit", 5),
                enable_advanced_search=parameters.get("enable_advanced_search", True),
                include_citations=parameters.get("include_citations", True),
                max_chars=parameters.get("max_chars", 1200),
                namespace=parameters.get("namespace", "default")
            )
        elif action == "search":
            return self._search(
                query=parameters.get("query") or parameters.get("question"),
                limit=parameters.get("limit", 5),
                min_score=parameters.get("min_score", 0.1),
                enable_advanced_search=parameters.get("enable_advanced_search", True),
                max_chars=parameters.get("max_chars", 1200),
                include_citations=parameters.get("include_citations", True),
                namespace=parameters.get("namespace", "default")
            )
        elif action == "stats":
            return self._get_stats(namespace=parameters.get("namespace", "default"))
        elif action == "clear":
            return self._clear_knowledge_base(
                confirm=parameters.get("confirm", False),
                namespace=parameters.get("namespace", "default")
            )
        else:
            return f"è¿è¡Œå‡ºé”™ï¼šä¸æ”¯æŒçš„æ“ä½œ{action}"

    def get_parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="action",
                type="string",
                description="æ“ä½œç±»å‹ï¼šadd_document(æ·»åŠ æ–‡æ¡£), add_text(æ·»åŠ æ–‡æœ¬), ask(æ™ºèƒ½é—®ç­”), search(æœç´¢), stats(ç»Ÿè®¡), clear(æ¸…ç©º)",
                required=True
            ),
            ToolParameter(
                name="file_path",
                type="string",
                description="æ–‡æ¡£æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒPDFã€Wordã€Excelã€PPTã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰å¤šç§æ ¼å¼ï¼‰",
                required=False
            ),
            ToolParameter(
                name="text",
                type="string",
                description="è¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹",
                required=False
            ),
            ToolParameter(
                name="question",
                type="string", 
                description="ç”¨æˆ·é—®é¢˜ï¼ˆç”¨äºæ™ºèƒ½é—®ç­”ï¼‰",
                required=False
            ),
            ToolParameter(
                name="query",
                type="string",
                description="æœç´¢æŸ¥è¯¢è¯ï¼ˆç”¨äºåŸºç¡€æœç´¢ï¼‰",
                required=False
            ),
            ToolParameter(
                name="namespace",
                type="string",
                description="çŸ¥è¯†åº“å‘½åç©ºé—´ï¼ˆç”¨äºéš”ç¦»ä¸åŒé¡¹ç›®ï¼Œé»˜è®¤ï¼šdefaultï¼‰",
                required=False,
                default="default"
            ),
            ToolParameter(
                name="limit",
                type="integer",
                description="è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ï¼š5ï¼‰",
                required=False,
                default=5
            ),
            ToolParameter(
                name="include_citations",
                type="boolean",
                description="æ˜¯å¦åŒ…å«å¼•ç”¨æ¥æºï¼ˆé»˜è®¤ï¼štrueï¼‰",
                required=False,
                default=True
            )
        ]

    @tool_action("rag_add_document", "æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆæ”¯æŒPDFã€Wordã€Excelã€PPTã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰å¤šç§æ ¼å¼ï¼‰")
    def _add_document(
        self,
        file_path: str,
        document_id: str = None,
        namespace: str = "default",
        chunk_size: int = 800,
        chunk_overlap: int = 100
    ) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“

        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
            namespace: çŸ¥è¯†åº“å‘½åç©ºé—´ï¼ˆç”¨äºéš”ç¦»ä¸åŒé¡¹ç›®ï¼‰
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°

        Returns:
            æ‰§è¡Œç»“æœ
        """
        if not file_path or not os.path.exists(file_path):
            return f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}"
        pipeline = self._get_pipeline(namespace)
        t0 = time.time()
        chunks_added = pipeline["add_documents"](
            file_paths=[file_path],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        t1 = time.time()
        process_ms = int((t1 - t0) * 1000)
        if chunks_added == 0:
            return f"æœªèƒ½ä»æ–‡ä»¶è§£æå†…å®¹ï¼š{os.path.basename(file_path)}"      
        return (
            f"æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼š{os.path.basename(file_path)}\n"
            f"åˆ†å—æ•°é‡ï¼š{chunks_added}\n"
            f"å¤„ç†æ—¶é—´ï¼š{process_ms}ms\n"
            f"å‘½åç©ºé—´ï¼š{pipeline.get('namespace', self.rag_namespace)}")
    
    @tool_action("rag_add_text", "æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“")
    def _add_text(
        self,
        text: str,
        document_id: str = None,
        namespace: str = "default",
        chunk_size: int = 800,
        chunk_overlap: int = 100
    ) -> str:
        """æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“

        Args:
            text: è¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹
            document_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
            namespace: çŸ¥è¯†åº“å‘½åç©ºé—´
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°

        Returns:
            æ‰§è¡Œç»“æœ
        """
        metadata = None
        if not text or not text.strip():
            return "è¾“å…¥çš„æ–‡æœ¬å†…å®¹ä¸ºç©º"
        document_id = document_id or f"text_{abs(hash(text)) % 100000}"
        tmp_path = os.path.join(self.knowledge_base_path, f"{document_id}.md")
        with open(tmp_path, 'w', encoding="utf-8", errors="ignore") as f:
            f.write(text)
        pipeline = self._get_pipeline(namespace)
        t0 = time.time()
        chunks_added = pipeline["add_documents"](
            file_paths=[tmp_path],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        t1 = time.time()
        process_ms = int((t1 - t0) * 1000)
        if os.path.exists(tmp_path):
            os.remove(tmp_path)
        if chunks_added == 0:
            return f"æœªèƒ½ä»æ–‡æœ¬ç”Ÿæˆæœ‰æ•ˆåˆ†å—"
        return (
            f"æ–‡æœ¬å·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼š{document_id}\n"
            f"åˆ†å—æ•°é‡ï¼š{chunks_added}\n"
            f"å¤„ç†æ—¶é—´ï¼š{process_ms}ms\n"
            f"å‘½åç©ºé—´ï¼š{pipeline.get('namespace', self.rag_namespace)}")
    
    @tool_action("rag_search", "æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹")
    def _search(
        self,
        query: str,
        limit: int = 5,
        min_score: float = 0.1,
        enable_advanced_search: bool = True,
        max_chars: int = 1200,
        include_citations: bool = True,
        namespace: str = "default"
    ) -> str:
        """æœç´¢çŸ¥è¯†åº“

        Args:
            query: æœç´¢æŸ¥è¯¢è¯
            limit: è¿”å›ç»“æœæ•°é‡
            min_score: æœ€ä½ç›¸å…³åº¦åˆ†æ•°
            enable_advanced_search: æ˜¯å¦å¯ç”¨é«˜çº§æœç´¢ï¼ˆMQEã€HyDEï¼‰
            max_chars: æ¯ä¸ªç»“æœæœ€å¤§å­—ç¬¦æ•°
            include_citations: æ˜¯å¦åŒ…å«å¼•ç”¨æ¥æº
            namespace: çŸ¥è¯†åº“å‘½åç©ºé—´

        Returns:
            æœç´¢ç»“æœ
        """
        if not query or not query.strip():
            return "è¾“å…¥çš„æŸ¥è¯¢ä¸ºç©º"
        pipeline = self._get_pipeline(namespace)
        if enable_advanced_search:
            results = pipeline["search_advanced"](
                query=query,
                top_k=limit,
                enable_mqe=True,
                enable_hyde=True,
                score_threshold=min_score if min_score > 0 else None)
        else:
            results = pipeline["search"](
                query=query,
                top_k=limit,
                score_threshold=min_score if min_score > 0 else None)
        if not results:
            return f"æœªæ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„å†…å®¹"
        search_result = ["æœç´¢ç»“æœï¼š"]
        for i, result in enumerate(results, 1):
            meta = result.get("metadata", {})
            score = result.get("score", 0.0)
            content = meta.get("content", "")[:200] + "..."
            source = meta.get("source_path", "unknown")
            
            def clean_text(text):
                try:
                    return str(text).encode("utf-8", errors="ignore").decode("utf-8")
                except Exception:
                    return str(text)

            clean_content = clean_text(content)
            clean_source = clean_text(source)
            search_result.append(f"\n{i}. æ–‡æ¡£ï¼š**{clean_source}** ï¼ˆç›¸ä¼¼åº¦ï¼š{score:.3f}ï¼‰")
            search_result.append(f"   {clean_content}")
            if include_citations and meta.get("heading_path"):
                clean_heading = clean_text(str(meta["heading_path"]))
                search_result.append(f"   ç« èŠ‚ï¼š{clean_heading}")
        return "\n".join(search_result)
    
    @tool_action("rag_ask", "åŸºäºçŸ¥è¯†åº“è¿›è¡Œæ™ºèƒ½é—®ç­”")
    def _ask(
        self,
        question: str,
        limit: int = 5,
        enable_advanced_search: bool = True,
        include_citations: bool = True,
        max_chars: int = 1200,
        namespace: str = "default"
    ) -> str:
        """æ™ºèƒ½é—®ç­”ï¼šæ£€ç´¢ â†’ ä¸Šä¸‹æ–‡æ³¨å…¥ â†’ LLMç”Ÿæˆç­”æ¡ˆ

        Args:
            question: ç”¨æˆ·é—®é¢˜
            limit: æ£€ç´¢ç»“æœæ•°é‡
            enable_advanced_search: æ˜¯å¦å¯ç”¨é«˜çº§æœç´¢
            include_citations: æ˜¯å¦åŒ…å«å¼•ç”¨æ¥æº
            max_chars: æ¯ä¸ªç»“æœæœ€å¤§å­—ç¬¦æ•°
            namespace: çŸ¥è¯†åº“å‘½åç©ºé—´

        Returns:
            æ™ºèƒ½é—®ç­”ç»“æœ

        æ ¸å¿ƒæµç¨‹:
        1. è§£æç”¨æˆ·é—®é¢˜
        2. æ™ºèƒ½æ£€ç´¢ç›¸å…³å†…å®¹
        3. æ„å»ºä¸Šä¸‹æ–‡å’Œæç¤ºè¯
        4. LLMç”Ÿæˆå‡†ç¡®ç­”æ¡ˆ
        5. æ·»åŠ å¼•ç”¨æ¥æº
        """
        if not question or not question.strip():
            return "è¾“å…¥çš„é—®é¢˜ä¸ºç©º"
        user_question = question.strip()
        print(f"[RAGTool] æ™ºèƒ½é—®ç­”ï¼š{user_question}")
        pipeline = self._get_pipeline(namespace)
        search_start = time.time()
        if enable_advanced_search:
            results = pipeline["search_advanced"](
                query=user_question,
                top_k=limit,
                enable_mqe=True,
                enable_hyde=True)
        else:
            results = pipeline["search"](
                query=user_question,
                top_k=limit)
        search_time = int((time.time() - search_start) * 1000)
        if not results:
            return (
                f"æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{user_question}ã€ç›¸å…³çš„ä¿¡æ¯ã€‚\n\n"
                f"å»ºè®®ï¼š\n"
                f"â€¢ å°è¯•ä½¿ç”¨æ›´ç®€æ´çš„å…³é”®è¯\n"
                f"â€¢ æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ ç›¸å…³æ–‡æ¡£\n"
                f"â€¢ ä½¿ç”¨ stats æ“ä½œæŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€"
            )
        context_parts = []
        citations = []
        total_score = 0
        for i, result in enumerate(results):
            meta = result.get("metadata", {})
            content = meta.get("content", "").strip()
            source = meta.get("source_path", "unknown")
            score = result.get("score", 0.0)
            total_score += score
            if content:
                cleaned_content = self._clean_content_for_context(content)
                context_parts.append(f"ç‰‡æ®µ {i+1}ï¼š{cleaned_content}")
                if include_citations:
                    citations.append({
                        "index": i+1,
                        "source": os.path.basename(source),
                        "score": score
                    })
        context = "\n\n".join(context_parts)
        if len(context) > max_chars:
            context = self._smart_truncate_context(context, max_chars)
        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(user_question, context)
        enhanced_prompt = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        llm_start = time.time()
        answer = self.llm.invoke(enhanced_prompt)
        llm_time = int((time.time() - llm_start) * 1000)
        if not answer or not answer.strip():
            return "LLMæœªèƒ½ç”Ÿæˆæœ‰æ•ˆç­”æ¡ˆï¼Œè¯·ç¨åé‡è¯•"
        final_answer = self._format_final_answer(
                question=user_question,
                answer=answer.strip(),
                citations=citations if include_citations else None,
                search_time=search_time,
                llm_time=llm_time,
                avg_score=total_score / len(results) if results else 0
            )
        return final_answer

    def _clean_content_for_context(self, content: str) -> str:
        content = " ".join(content.split())
        if len(content) > 300:
            content = content[:300] + "..."
        return content
    
    def _smart_truncate_context(self, context: str, max_chars: int) -> str:
        if len(context) <= max_chars:
            return context
        truncated = context[:max_chars]
        last_break = truncated.rfind("\n\n")
        if last_break > max_chars * 0.7: 
            return truncated[:last_break] + "\n\n[...æ›´å¤šå†…å®¹è¢«æˆªæ–­]"
        else:
            return truncated[:max_chars-20] + "...[å†…å®¹è¢«æˆªæ–­]"
    
    def _build_system_prompt(self) -> str:
        return (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š\n"
            "1. ç²¾å‡†ç†è§£ï¼šä»”ç»†ç†è§£ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ„å›¾\n"
            "2. å¯ä¿¡å›ç­”ï¼šä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ï¼Œä¸ç¼–é€ å†…å®¹\n"
            "3. ä¿¡æ¯æ•´åˆï¼šä»å¤šä¸ªç‰‡æ®µä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå½¢æˆå®Œæ•´ç­”æ¡ˆ\n"
            "4. æ¸…æ™°è¡¨è¾¾ï¼šç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”ï¼Œé€‚å½“ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼\n"
            "5. è¯šå®è¡¨è¾¾ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦è¯šè¯´æ˜\n\n"
            "å›ç­”æ ¼å¼è¦æ±‚ï¼š\n"
            "â€¢ ç›´æ¥å›ç­”æ ¸å¿ƒé—®é¢˜\n"
            "â€¢ å¿…è¦æ—¶ä½¿ç”¨è¦ç‚¹æˆ–æ­¥éª¤\n"
            "â€¢ å¼•ç”¨å…³é”®åŸæ–‡æ—¶ä½¿ç”¨å¼•å·\n"
            "â€¢ é¿å…é‡å¤å’Œå†—ä½™"
        )
    
    def _build_user_prompt(self, question: str, context: str) -> str:
        return (
            f"è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n\n"
            f"ã€é—®é¢˜ã€‘{question}\n\n"
            f"ã€ç›¸å…³ä¸Šä¸‹æ–‡ã€‘\n{context}\n\n"
            f"ã€è¦æ±‚ã€‘è¯·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜éœ€è¦ä»€ä¹ˆé¢å¤–ä¿¡æ¯ã€‚"
        )
    
    def _format_final_answer(self, question: str, answer: str, citations: Optional[List[Dict]] = None, 
            search_time: int = 0, llm_time: int = 0, avg_score: float = 0) -> str:
        result = [f"**æ™ºèƒ½é—®ç­”ç»“æœ**\n"]
        result.append(answer)
        if citations:
            result.append("\n\n**å‚è€ƒæ¥æº**")
            for citation in citations:
                result.append(f"[{citation['index']}] {citation['source']} ï¼ˆç›¸ä¼¼åº¦: {citation['score']:.3f}ï¼‰")
        result.append(f"\n===== æ£€ç´¢ï¼š{search_time}ms | ç”Ÿæˆï¼š{llm_time}ms | å¹³å‡ç›¸ä¼¼åº¦ï¼š{avg_score:.3f} =====")
        return "\n".join(result)

    @tool_action("rag_clear", "æ¸…ç©ºçŸ¥è¯†åº“ï¼ˆå±é™©æ“ä½œï¼Œè¯·è°¨æ…ä½¿ç”¨ï¼‰")
    def _clear_knowledge_base(self, confirm: bool = False, namespace: str = "default") -> str:
        """æ¸…ç©ºçŸ¥è¯†åº“

        Args:
            confirm: ç¡®è®¤æ‰§è¡Œï¼ˆå¿…é¡»è®¾ç½®ä¸ºTrueï¼‰
            namespace: çŸ¥è¯†åº“å‘½åç©ºé—´

        Returns:
            æ‰§è¡Œç»“æœ
        """
        if not confirm:
            return ("[RAGTool] âš ï¸\x20\x20å±é™©æ“ä½œï¼šæ¸…ç©ºçŸ¥è¯†åº“å°†åˆ é™¤æ‰€æœ‰æ•°æ®ï¼\n"
                    "[RAGTool] ğŸ’¡\x20è¯·ä½¿ç”¨ confirm=true å‚æ•°ç¡®è®¤æ‰§è¡Œã€‚")
        pipeline = self._get_pipeline(namespace)
        store = pipeline.get("store")
        namespace_id = pipeline.get("namespace", self.rag_namespace)
        success = store.clear_collection() if store else False
        if success:
            self._pipelines[namespace_id] = create_rag_pipeline(
                collection_name=self.collection_name,
                rag_namespace=namespace_id)
            return f"çŸ¥è¯†åº“å·²æˆåŠŸæ¸…ç©ºï¼ˆå‘½åç©ºé—´ï¼š{namespace_id}ï¼‰"
        else:
            return "æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥"

    @tool_action("rag_stats", "è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯")
    def _get_stats(self, namespace: str = "default") -> str:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡

        Args:
            namespace: çŸ¥è¯†åº“å‘½åç©ºé—´

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        pipeline = self._get_pipeline(namespace)
        stats = pipeline["get_stats"]()
        stats_info = [
                "**RAG çŸ¥è¯†åº“ç»Ÿè®¡**",
                f"å‘½åç©ºé—´ï¼š{pipeline.get('namespace', self.rag_namespace)}",
                f"é›†åˆåç§°ï¼š{self.collection_name}",
                f"å­˜å‚¨æ ¹è·¯å¾„ï¼š{self.knowledge_base_path}"]
        if stats:
            store_type = stats.get("store_type", "unknown")
            total_vectors = (
                stats.get("points_count") or 
                stats.get("vectors_count") or 
                stats.get("count") or 0
            )
            stats_info.extend([
                f"å­˜å‚¨ç±»å‹ï¼š{store_type}",
                f"æ–‡æ¡£åˆ†å—æ•°ï¼š{int(total_vectors)}",
            ])  
            if "config" in stats:
                config = stats["config"]
                if isinstance(config, dict):
                    vector_size = config.get("vector_size", "unknown")
                    distance = config.get("distance", "unknown")
                    stats_info.extend([
                        f"å‘é‡ç»´åº¦ï¼š{vector_size}",
                        f"è·ç¦»åº¦é‡ï¼š{distance}"
                    ])
        stats_info.extend([
            "\n**ç³»ç»ŸçŠ¶æ€**",
            f"RAG ç®¡é“ï¼š{'æ­£å¸¸' if self.initialized else 'å¼‚å¸¸'}",
            f"LLM è¿æ¥ï¼š{'æ­£å¸¸' if hasattr(self, 'llm') else 'å¼‚å¸¸'}"
        ])
        return "\n".join(stats_info)

    def get_relevant_context(self, query: str, limit: int = 3, max_chars: int = 1200, namespace: Optional[str] = None) -> str:
        if not query.strip():
            return ""
        pipeline = self._get_pipeline(namespace)
        results = pipeline["search"](
            query=query,
            top_k=limit
        )
        if not results:
            return ""
        context_parts = []
        for result in results:
            content = result.get("metadata", {}).get("content", "")
            if content:
                context_parts.append(content)
        merged_context = "\n\n".join(context_parts)
        if len(merged_context) > max_chars:
            merged_context = merged_context[:max_chars] + "..."
        return merged_context
    
    def clear_all_namespaces(self) -> str:
        for ns, pipeline in self._pipelines.items():
            store = pipeline.get("store")
            if store:
                store.clear_collection()
        self._pipelines.clear()
        self._init_components()
        return "æ‰€æœ‰å‘½åç©ºé—´æ•°æ®å·²æ¸…ç©ºå¹¶é‡æ–°åˆå§‹åŒ–"
    
    def add_document(self, file_path: str, namespace: str = "default") -> str:
        return self.run({
            "action": "add_document",
            "file_path": file_path,
            "namespace": namespace
        })
    
    def add_text(self, text: str, namespace: str = "default", document_id: str = None) -> str:
        return self.run({
            "action": "add_text",
            "text": text,
            "namespace": namespace,
            "document_id": document_id
        })
    
    def ask(self, question: str, namespace: str = "default", **kwargs) -> str:
        params = {
            "action": "ask",
            "question": question,
            "namespace": namespace
        }
        params.update(kwargs)
        return self.run(params)
    
    def search(self, query: str, namespace: str = "default", **kwargs) -> str:
        params = {
            "action": "search",
            "query": query,
            "namespace": namespace
        }
        params.update(kwargs)
        return self.run(params)
    
    def add_documents_batch(self, file_paths: List[str], namespace: str = "default") -> None:
        if not file_paths:
            return "è¾“å…¥çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸ºç©º"
        for i, file_path in enumerate(file_paths, 1):
            print(f"[RAGTool] ----- å¤„ç†æ–‡æ¡£ {i}/{len(file_paths)}ï¼š{os.path.basename(file_path)}")
            result = self.add_document(file_path, namespace)
            print(result)
    
    def add_texts_batch(self, texts: List[str], namespace: str = "default", document_ids: Optional[List[str]] = None) -> str:
        if not texts:
            return "è¾“å…¥çš„æ–‡æœ¬åˆ—è¡¨ä¸ºç©º"
        if document_ids and len(document_ids) != len(texts):
            return "æ–‡æœ¬æ•°é‡å’Œæ–‡æ¡£IDæ•°é‡ä¸åŒ¹é…"
        for i, text in enumerate(texts):
            doc_id = document_ids[i] if document_ids else f"batch_text_{i+1}"
            print(f"[RAGTool] ----- å¤„ç†æ–‡æœ¬ {i+1}/{len(texts)}ï¼š{doc_id}")
            result = self.add_text(text, namespace, doc_id)
            print(result)
